# import argparse
import utils as ut
import os
import s3fs
import numpy as np
import dgl
import networkx as nx
import pickle
import warnings
import torch
from torch import sparse
import numpy as np
warnings.filterwarnings("ignore")
seed_list = list(range(3407, 10000, 10))


def describe_dgl_graph(g, name, max_examples=5):
    print(f"ğŸ“Š RÃ©sumÃ© du graphe DGL du jeu de donnÃ©es {name}")
    print("-" * 40)
    print(f"Nombre de nÅ“uds : {g.num_nodes()}")
    print(f"Nombre d'arÃªtes : {g.num_edges()}")

    nxg = g.to_networkx()
    print("Le graphe est-il orientÃ© ?", nxg.is_directed())

    # Affichage de quelques infos de base
    print(f"Node feature shape: {g.ndata['feature'].shape}")
    print(f"Label present: {'label' in g.ndata}")

    print("\nğŸ”‘ Attributs des nÅ“uds :")
    for key in g.ndata.keys():
        print(f" - {key}: shape = {g.ndata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.ndata[key][:max_examples]}")

    print("\nğŸ”‘ Attributs des arÃªtes :")
    for key in g.edata.keys():
        print(f" - {key}: shape = {g.edata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.edata[key][:max_examples]}")

    print("\nğŸ§ª Masques (sâ€™ils existent) :")
    for mask in ['train_mask', 'val_mask', 'test_mask']:
        if mask in g.ndata:
            print(f" - {mask} â†’ {g.ndata[mask].sum().item()} nÅ“uds")

    print("\nğŸ” Quelques arÃªtes :")
    src, dst = g.edges()
    for i in range(min(max_examples, len(src))):
        print(f"   {src[i].item()} â†’ {dst[i].item()}")
    '''
    print(g.etypes)
    for etype in g.canonical_etypes:
        print(f"{etype} features:", g.edges[etype].data.keys())
    print(g.edges[('_N', '_E', '_N')].data['count'])
    for etype in g.canonical_etypes:
        for key in g.edges[etype].data.keys():
            print(f"{etype} - {key}:")
            print(g.edges[etype].data[key])

    src, dst = g.edges(form='uv', etype=('_N', '_E', '_N'))
    counts = g.edges[('_N', '_E', '_N')].data['count']

    for i, (u, v, c) in enumerate(zip(src, dst, counts)):
        if i >= 100:
            break
        print(f"{u.item()} -> {v.item()} : count = {c.item()}")
    '''
    '''
    # 1. RÃ©cupÃ©rer les arÃªtes et la feature 'count'
    src, dst = g.edges(form='uv', etype=('_N', '_E', '_N'))
    counts = g.edges[('_N', '_E', '_N')].data['count']          # (num_edges, 1) ou (num_edges,)

# 2. Construire un masque boolÃ©en : True si count > 1
    mask = (counts.squeeze() > 1)   # .squeeze() enlÃ¨ve la dimension inutile si besoin

# 3. Appliquer le masque pour ne garder que les arÃªtes voulues
    src_keep = src[mask]
    dst_keep = dst[mask]
    count_keep = counts[mask]

# 4. Afficher les arÃªtes filtrÃ©es
    for u, v, c in zip(src_keep, dst_keep, count_keep):
        print(f"{u.item()} -> {v.item()} : count = {c.item()}")
    
    for ntype in g.ntypes:
        print(f"Attributs des nÅ“uds de type {ntype} :")
        print(list(g.nodes[ntype].data.keys()))

    '''


datasets = ['reddit', 'weibo']


# Boucle sur tous les datasets
for dataset_name in datasets:
    # Chargement du dataset avec GADBench
    data = ut.Dataset(name=dataset_name, prefix='GADBench/datasets/')
    g = data.graph  # RÃ©cupÃ©ration du graphe DGL

    describe_dgl_graph(g, dataset_name, 2)

    # ================================
    # 1. Extraction des features des nÅ“uds
    # ================================
    # Passage des features en numpy pour export
    x = g.ndata['feature'].cpu().numpy()

    # ================================
    # 2. Extraction des Ã©tiquettes des nÅ“uds
    # ================================
    if 'label' in g.ndata:
        # Si les labels sont prÃ©sents, on les extrait
        y = g.ndata['label'].cpu().numpy()
    else:
        # Sinon, on utilise -1 pour indiquer l'absence d'Ã©tiquette
        y = np.full(g.num_nodes(), fill_value=-1)

    # ================================
    # 3. CrÃ©ation de la matrice de poids des arÃªtes
    # ================================
    num_nodes = g.num_nodes()

    # Initialisation d'une matrice (num_nodes x num_nodes) remplie de zÃ©ros
    A = np.zeros((num_nodes, num_nodes))

    # RÃ©cupÃ©ration des arÃªtes (liste des paires source â†’ destination)
    src, dst = g.edges()
    src = src.cpu().numpy()
    dst = dst.cpu().numpy()

    # Si les arÃªtes ont un attribut 'weight', on lâ€™utilise ; sinon poids unitaire
    if 'count' in g.edata:
        count = g.edata['count']         # tensor shape: [N, 1]
        count = count.squeeze()          # shape devient [N]
        count = count.cpu().numpy()      # devient array([1., 2., ...])
    else:
        count = np.ones(len(src))  # poids par dÃ©faut = 1

    # Remplissage de la matrice de similaritÃ©s avec les poids
    for s, d, w in zip(src, dst, count):
        A[s, d] = w
        # A[d, s] = w  # si le graphe est non orientÃ© (symÃ©trique)
    print(f"matrice d'adjacence : {A}")

    if np.allclose(A.T, A):
        print("matrice symÃ©trique")
    else:
        print("matrice pas symÃ©trique")

    print(f"Le graphe est-il homogÃ¨ne ? {g.is_homogeneous}")

# Autre mÃ©thode pour matrice d'adjacence
    # Construire la matrice sparse pondÃ©rÃ©e
    # Si src et dst sont des numpy arrays, on les convertit en torch tensors
    src_tensor = torch.tensor(src) if not torch.is_tensor(src) else src
    dst_tensor = torch.tensor(dst) if not torch.is_tensor(dst) else dst

    adj = torch.sparse_coo_tensor(
        indices=torch.stack([src_tensor, dst_tensor]),
        values=count,
        size=(num_nodes, num_nodes)
    )

# adj contient les poids des arÃªtes
    print(f"matrice d'adjacence autre mÃ©thode de calcul : {adj.to_dense()}")  # Affiche la matrice dense
    # Si adj est sparse
    dense_adj = adj.to_dense()

    # Convertir en numpy array
    np_adj = dense_adj.cpu().numpy()
    if np.allclose(np_adj.T,np_adj):
        print("matrice symÃ©trique")
    else:
        print("matrice pas symÃ©trique")





    BUCKET = "sophieperrinlyon2"
    PREFIX = "albert/"

    fs = s3fs.S3FileSystem()

    for name, arr in [(f"x_{dataset_name}.npy", x), (f"y_{dataset_name}.npy", y), (f"A_{dataset_name}.npy", A)]:
        path = f"{BUCKET}/{PREFIX}{name}"
        with fs.open(path, "wb") as f:
            np.save(f, arr)
            print(f"  âœ” Uploaded {name}")




'''
    # 1. Reconstruire le masque users/items
# Ici j'utilise le fait que seuls les users ont un label >= 0.
    labels = g.ndata['label']               # shape [10984]
    is_user = labels >= 0                   # True = user, False = item
    is_item = ~is_user

# 2. Comptage des nÅ“uds de chaque type
    num_users = is_user.sum().item()
    num_items = is_item.sum().item()
    print(f"Users dÃ©tectÃ©s : {num_users}")
    print(f"Items dÃ©tectÃ©s : {num_items}")

# 3. RÃ©cupÃ©rer toutes les arÃªtes
    src, dst = g.edges()

# 4. Compter les 4 cas de figure
    mask_ui = is_user[src] & is_item[dst]    # user â†’ item
    mask_iu = is_item[src] & is_user[dst]    # item â†’ user
    mask_uu = is_user[src] & is_user[dst]    # user â†’ user
    mask_ii = is_item[src] & is_item[dst]    # item â†’ item

    counts = {
        "userâ†’item" : mask_ui.sum().item(),
        "itemâ†’user" : mask_iu.sum().item(),
        "userâ†’user" : mask_uu.sum().item(),
        "itemâ†’item" : mask_ii.sum().item(),
        "total edges": src.shape[0]
    }

    for k, v in counts.items():
        print(f"{k:10s} : {v}")

# 5. Pourcentage userâ†’item
    pct_ui = counts["userâ†’item"] / counts["total edges"] * 100
    print(f"\n% userâ†’item  : {pct_ui:.2f}%")
'''
'''
import os
import s3fs
import numpy as np

#BUCKET_OUT = "sophieperrinlyon2"


BUCKET = "sophieperrinlyon2"
PREFIX = "albert/"

fs = s3fs.S3FileSystem()

x_reddit = np.load("x_reddit.npy")
y_reddit = np.load("y_reddit.npy")
A_reddit = np.load("A_reddit.npy")


for name, arr in [("x_reddit.npy", x_reddit), ("y_reddit.npy", y_reddit), ("A_reddit.npy", A_reddit)]:
    path = f"{BUCKET}/{PREFIX}{name}"
    with fs.open(path, "wb") as f:
        np.save(f, arr)
    print(f"  âœ” Uploaded {name}")
'''
'''
    # ================================
    # 4. Sauvegarde dans un fichier .npz compressÃ©
    # ================================
    # Nettoyage du nom de fichier
    graph_name = dataset_name.replace("/", "_")

    # Enregistrement dans un fichier compressÃ© contenant x, y, similarities
    np.savez_compressed(
        os.path.join(output_dir, f"{graph_name}.npz"),
        x=x,
        y=y,
        A=A
    )

'''

'''  
    # sauvegarder les features ou graphes
    X = g.ndata['feature'].cpu().numpy()
    np.save(f"{dataset_name}_features.npy", X)
    # convertir en networkx
    nx_graph = dgl.to_networkx(g)
    with open(f"{dataset_name}.gpickle", "wb") as f:
        pickle.dump(nx_graph, f)

'''



'''  
    # sauvegarder les features ou graphes
    X = g.ndata['feature'].cpu().numpy()
    np.save(f"{dataset_name}_features.npy", X)
    # convertir en networkx
    nx_graph = dgl.to_networkx(g)
    with open(f"{dataset_name}.gpickle", "wb") as f:
        pickle.dump(nx_graph, f)

'''
