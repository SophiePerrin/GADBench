# import argparse
import utils as ut
import os
import s3fs
import numpy as np
import dgl
import networkx as nx
import pickle
import warnings
import torch
from torch import sparse
import numpy as np
warnings.filterwarnings("ignore")
seed_list = list(range(3407, 10000, 10))
import torch
import numpy as np
from sklearn.decomposition import PCA
import matplotlib 
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering

from sklearn.preprocessing import normalize
from scipy.sparse.csgraph import laplacian
from numpy.linalg import eigvalsh
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

from joblib import Parallel, delayed

#############################################

# Fonctions pour l'√©tude des caract√©ristiques des graphes de donn√©es

#############################################


def analyser_aretes(g, poids_key='count'): # fonction utilis√©e tout √† la fin de la fonction describe_dgl_graph()
    src, dst = g.edges()
    weights = g.edata[poids_key]

    num_edges = len(src)
    print(f"\nüî¢ Nombre total d‚Äôar√™tes : {num_edges}")

    # Auto-boucles et ar√™tes entre n≈ìuds diff√©rents
    mask_self = src == dst
    mask_diff = src != dst

    weights_self = weights[mask_self]
    weights_diff = weights[mask_diff]

    print(f"üîÅ Auto-boucles : {len(weights_self)} ar√™tes, poids min = {weights_self.min().item() if len(weights_self) > 0 else None}, max = {weights_self.max().item() if len(weights_self) > 0 else None}")
    print(f"üîó Ar√™tes entre n≈ìuds diff√©rents : {len(weights_diff)} ar√™tes, poids min = {weights_diff.min().item() if len(weights_diff) > 0 else None}, max = {weights_diff.max().item() if len(weights_diff) > 0 else None}")

    # Conversion CPU pour comparaison set
    src_np = src.cpu().numpy()
    dst_np = dst.cpu().numpy()
    weights_np = weights.cpu().numpy()

    edges_np = np.stack((src_np, dst_np), axis=1)
    edge_set = set(map(tuple, edges_np))

    sym_mask = np.array([(j, i) in edge_set for i, j in edges_np])
    asym_mask = ~sym_mask

    weights_sym = weights_np[sym_mask]
    weights_asym = weights_np[asym_mask]

    print(f"üîÑ Ar√™tes avec ar√™te inverse : {len(weights_sym)} ar√™tes, poids min = {weights_sym.min() if len(weights_sym) > 0 else None}, max = {weights_sym.max() if len(weights_sym) > 0 else None}")
    print(f"‚Ü™Ô∏è Ar√™tes sans ar√™te inverse : {len(weights_asym)} ar√™tes, poids min = {weights_asym.min() if len(weights_asym) > 0 else None}, max = {weights_asym.max() if len(weights_asym) > 0 else None}")


def describe_dgl_graph(g, name, max_examples=5):
    print(f"üìä R√©sum√© du graphe DGL du jeu de donn√©es {name}")
    print("-" * 40)
    print(f"Nombre de n≈ìuds : {g.num_nodes()}")
    print(f"Nombre d'ar√™tes : {g.num_edges()}")
    print(g)

    nxg = g.to_networkx()
    print("Le graphe est-il orient√© ?", nxg.is_directed())

    # Affichage de quelques infos de base
    print(f"Node feature shape: {g.ndata['feature'].shape}")
    print(f"Label present: {'label' in g.ndata}")
    
    print("\nüîë Attributs des n≈ìuds :")
    for key in g.ndata.keys():
        print(f" - {key}: shape = {g.ndata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.ndata[key][:max_examples]}")
    
    print("\nüîë Attributs des ar√™tes :")
    for key in g.edata.keys():
        print(f" - {key}: shape = {g.edata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.edata[key][:max_examples]}")

    print("\nüß™ Masques (s‚Äôils existent) :")
    for mask in ['train_mask', 'val_mask', 'test_mask']:
        if mask in g.ndata:
            print(f" - {mask} ‚Üí {g.ndata[mask].sum().item()} n≈ìuds")
    
    print("\nüîÅ Quelques ar√™tes :")
    src, dst = g.edges()
    for i in range(min(max_examples, len(src))):
        print(f"   {src[i].item()} ‚Üí {dst[i].item()}")

    print(f"Le graphe est-il homog√®ne ? {g.is_homogeneous}")
    print(f"Le graphe est-il unibipartite ? {g.is_unibipartite}")
    print(f"R√©sultats de has nodes : {g.has_nodes}")

# Matrice d'adjacence
    # Construire la matrice sparse pond√©r√©e
    # Si src et dst sont des numpy arrays, on les convertit en torch tensors
    src_tensor = torch.tensor(src) if not torch.is_tensor(src) else src
    dst_tensor = torch.tensor(dst) if not torch.is_tensor(dst) else dst
    if 'count' in g.edata:
        count = g.edata['count']         # tensor shape: [N, 1]
        count = count.squeeze()          # shape devient [N]
    
    num_nodes = g.num_nodes()

    adj = torch.sparse_coo_tensor(
        indices=torch.stack([src_tensor, dst_tensor]),
        values=count,
        size=(num_nodes, num_nodes)
    )

# adj contient les poids des ar√™tes
    print(f"matrice d'adjacence : {adj.to_dense()}")  # Affiche la matrice dense
    # Si adj est sparse
    dense_adj = adj.to_dense()

    # Convertir en numpy array
    np_adj = dense_adj.cpu().numpy()
    if np.allclose(np_adj.T,np_adj):
        print("matrice sym√©trique")
    else:
        print("matrice pas sym√©trique")

###
    src, dst = g.edges()
    num_edges = len(src)

# Ar√™tes entre n≈ìuds diff√©rents
    mask_diff_nodes = src != dst
    num_diff_edges = mask_diff_nodes.sum().item()

    print(f"üî¢ Nombre total d'ar√™tes : {num_edges}")
    print(f"üîó Ar√™tes entre n≈ìuds diff√©rents : {num_diff_edges}")
    print(f"üîÅ Auto-boucles (i ‚Üí i) : {num_edges - num_diff_edges}")

    weights = g.edata['count']
    min_w = weights.min().item()
    max_w = weights.max().item()

    print(f"üìè Plage des poids d‚Äôar√™tes : min = {min_w}, max = {max_w}")

    # Cr√©er un ensemble des paires (i, j) et (j, i)
    edge_set = set(zip(src.tolist(), dst.tolist()))
    reverse_set = set((j, i) for (i, j) in edge_set if i != j)

    # Ar√™tes qui n'ont pas leur inverse
    asym_edges = reverse_set - edge_set
    print(len(asym_edges))
    if len(asym_edges) == 0:
        print("‚úÖ Le graphe est sym√©trique : pour chaque ar√™te i ‚Üí j, il existe j ‚Üí i.")
    else:
        print(f"‚ö†Ô∏è Le graphe est orient√© : {len(asym_edges)} ar√™tes n‚Äôont pas leur inverse.")
    analyser_aretes(g)

#############################################

# Fonctions pour la r√©duction de dimension des features des noeuds des graphes (lorsque utile)
# et pour transformer un graphe r√©ellement orient√© en graphe non orient√© (par la m√©thode de repond√©ration des arcs en ar√™tes)

#############################################


def analyze_feature_redundancy(graph, variance_thresh=1e-6, corr_thresh=0.95, pca_variance=0.95):
    # 1. Extraire les features
    X = graph.ndata['feature'].numpy()

    # 1. v√©rifier leurs caract√©ristiques
    print("Min global :", X.min())
    print("Max global :", X.max())
    
    norms = np.linalg.norm(X, axis=1)
    print("Norme moyenne :", norms.mean())
    print("Norme max :", norms.max())
    print("Ecart-type :", X.std(axis=0))

    means = X.mean(axis=0)
    stds = X.std(axis=0)

    print("Moyenne min (= 0 si d√©j√† centr√©e r√©duite):", means.min())
    print("Moyenne max (= 0 si d√©j√† centr√©e r√©duite):", means.max())
    print("√âcart-type min (= 1 si d√©j√† centr√©e r√©duite) :", stds.min())
    print("√âcart-type max (= 1 si d√©j√† centr√©e r√©duite) :", stds.max())


    # R√©sultat : ni reddit ni weibo ne sont centr√©s r√©duits, alors qu'ils doivent l'√™tre pour effectuer la PCA

    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)

    means = X.mean(axis=0)
    stds = X.std(axis=0)

    print("Moyenne min v√©rif (= 0 si centr√©e r√©duite):", means.min())
    print("Moyenne max v√©rif (= 0 si centr√©e r√©duite):", means.max())
    print("√âcart-type min v√©rif (= 1 si centr√©e r√©duite) :", stds.min())
    print("√âcart-type max v√©rif (= 1 si centr√©e r√©duite) :", stds.max())

    # 2. Calculer la variance
    variances = X.var(axis=0)
    var_idx = np.where(variances >= variance_thresh)[0]       # indices √† garder
    low_var_idx = np.where(variances < variance_thresh)[0]    # indices supprim√©s pour tra√ßabilit√©
    print(f"{len(low_var_idx)} features ont une variance < {variance_thresh} : {low_var_idx.tolist()} ‚Äî elles sont supprim√©es avant la PCA")

    # 3. Filtrer les colonnes √† faible variance
    X_clean = X[:, var_idx]

    # 4. Mettre √† jour les features du graphe
    graph.ndata['feature'] = torch.tensor(X_clean, dtype=torch.float32)

    # 5. Features tr√®s corr√©l√©es (calcul√© sur X d'origine, pas X_clean)
    corr_matrix = np.corrcoef(X, rowvar=False)
    np.fill_diagonal(corr_matrix, 0)
    
    # Matrice de corr√©lation absolue
    abs_corr = np.abs(corr_matrix)

    # Indices de la partie triangulaire sup√©rieure (hors diagonale)
    triu_indices = np.triu_indices_from(abs_corr, k=1)

    # Paires (i, j) avec leur valeur de corr√©lation
    pair_scores = [(i, j, abs_corr[i, j]) for i, j in zip(*triu_indices)]  # üîÑ remplac√© redundant_pairs

    # Trier par corr√©lation d√©croissante
    pair_scores.sort(key=lambda x: x[2], reverse=True)  # üîÑ nouveau : trie toutes les paires par corr√©lation

    # Affichage
    print("Top 10 des paires de features les plus corr√©l√©es :")  # üîÑ message plus clair
    for i, j, score in pair_scores[:10]:                         # üîÑ on affiche les paires r√©ellement les plus corr√©l√©es
        print(f"  Feature {i} ‚Üî Feature {j} (corr = {corr_matrix[i, j]:.2f})")

    # 6. PCA sur les features nettoy√©es
    pca = PCA(n_components=pca_variance)
    X_pca = pca.fit_transform(X_clean)
    print(f"PCA a r√©duit de {X_clean.shape[1]} √† {pca.n_components_} dimensions (variance expliqu√©e : {pca_variance})")

    means = X_pca.mean(axis=0)
    stds = X_pca.std(axis=0)

    print("Moyenne min v√©rif (= 0 si centr√©e r√©duite):", means.min())
    print("Moyenne max v√©rif (= 0 si centr√©e r√©duite):", means.max())
    print("√âcart-type min v√©rif (= 1 si centr√©e r√©duite) :", stds.min())
    print("√âcart-type max v√©rif (= 1 si centr√©e r√©duite) :", stds.max())

    # 7. Affichage des poids de la premi√®re composante
    comp_weights = np.abs(pca.components_[0])
    plt.bar(np.arange(len(comp_weights)), comp_weights)
    plt.title("Poids absolus des features dans la 1re composante principale")
    plt.xlabel("Feature index")
    plt.ylabel("Poids")
    plt.show()

    # 8. Remplacer les features du graphe par celles transform√©es par la PCA
    X_pca = pca.transform(X_clean)
    graph.ndata['feature'] = torch.tensor(X_pca, dtype=torch.float32)


    # 9. Retourner les r√©sultats
    return {
        'features_supprim√©es_par_variance': low_var_idx.tolist(),  
        'top_corr_pairs': pair_scores[:10],                   
        'pca_model': pca,
        'graph_pca': graph 
    }


def make_weighted_undirected_with_node_features(g):
    # 1. Extraire les ar√™tes orient√©es
    src, dst = g.edges()

    # 2. Compter les relations (non orient√©es)
    edge_counts = {}
    for u, v in zip(src.tolist(), dst.tolist()):
        key = tuple(sorted([u, v]))
        edge_counts[key] = edge_counts.get(key, 0) + 1

    # 3. Pr√©parer les ar√™tes et les poids
    new_src = []
    new_dst = []
    new_weights = []

    for (u, v), w in edge_counts.items():
        # Attribution du poids bas√© sur le type de relation
        if u == v:
            weight = 1.0  # boucle
        elif w == 1:
            weight = 0.5  # unidirectionnel
        else:
            weight = 1.0  # bidirectionnel

        # Ar√™te u ‚Üí v
        new_src.append(u)
        new_dst.append(v)
        new_weights.append(weight)

        # Ar√™te v ‚Üí u (sauf si boucle)
        if u != v:
            new_src.append(v)
            new_dst.append(u)
            new_weights.append(weight)

    # 4. Cr√©er le graphe non orient√©
    g_undir = dgl.graph((new_src, new_dst), num_nodes=g.num_nodes())

    # 5. Copier les features des n≈ìuds
    for key in g.ndata:
        g_undir.ndata[key] = g.ndata[key].clone()

    # 6. Ajouter les poids aux ar√™tes
    g_undir.edata['count'] = torch.tensor(new_weights, dtype=torch.float)

    return g_undir


#############################################

# #### Etude des caract√©ristiques des datasets et cr√©ation d'un dictionnaire pour pouvoir les manipuler s√©par√©ment ensuite :

#############################################

datasets = ['reddit', 'weibo']

graphs = {}  # Dictionnaire pour stocker les graphes

# Boucle sur tous les datasets
for dataset_name in datasets:
    # Chargement du dataset avec GADBench
    data = ut.Dataset(name=dataset_name, prefix='./datasets/')
    g = data.graph  # R√©cup√©ration du graphe DGL

    graphs[dataset_name] = g  # Stockage du graphe avec son nom

    describe_dgl_graph(g, dataset_name, 2)
    
graphs_modif = {} # Dictionnaire pour stocker les graphes apr√®s les modifications faites ci-dessous

#############################################

# Travail de transformation - adaptation du graphe de donn√©es reddit

# Ce graphe est sym√©trique, et chaque arc a un arc inverse. Dans DGL, tous les graphes sont de type orient√© (il est impossible qu'ils ne l'y soient pas).
# C'est du √† la sp√©cificit√© de DGL : faire des graphes pour y faire tourner des GNN.
# Notre graphe, √©tant sym√©trique, est donc d√©j√† sous la bonne forme en pratique (il n'est orient√© que parce que DGL lui attribue ce type).

# On a donc uniquement √† se pr√©occuper des features des noeuds, en √©liminant celles qui varient tr√®s faiblement entre les diff√©rents noeuds,
# puis en effectuant une ACP pour transformer nos features parfois tr√®s corr√©l√©es entre elles, en features orthogonales les unes aux autres, et moins nombreuses

#############################################

resultats_reddit = analyze_feature_redundancy(graphs['reddit'], pca_variance=0.95)

graphs_modif['reddit'] = resultats_reddit['graph_pca']

describe_dgl_graph(graphs_modif['reddit'], 'reddit_modif')

#############################################

# Travail de transformation - adaptation du graphe de donn√©es weibo

# weibo est r√©ellement orient√©, car de nombreux arcs n'ont pas d'arc retour : on cr√©e ces arcs retour
# et on proc√®de par repond√©ration : 
# 0 pour l'absence totale d'arc entre deux noeuds
# 1 pour un arc A --> B et B --> A
# 0,5 pour un arc uniquement A --> B (sans pr√©sence d'arc retour dans le graphe d'origine)

#############################################

graphs_modif['weibo'] = make_weighted_undirected_with_node_features(graphs['weibo'])

resultats_weibo = analyze_feature_redundancy(graphs_modif['weibo'], variance_thresh=1e-2, corr_thresh=0.95, pca_variance=0.99)

graphs_modif['weibo'] = resultats_weibo['graph_pca']

describe_dgl_graph(graphs_modif['weibo'], 'weibo_modif')

#############################################
for name, g in graphs_modif.items():
    print(name, g.num_nodes(), g.num_edges())

#############################################

# Export des noeuds+features (x), des labels (y), calcul de la matrice de similarit√© (issue de A remani√©e) des graphes de donn√©es
# pour le clustering spectral, et export de A pour utilisation par HypHC

#############################################

# fonctions utiles pour cette partie du programme


# Calcul de la similarit√© cosine entre features des noeuds par blocs (pour ne pas exploser la m√©moire dispo)
def compute_cosine_similarity_matrix_blockwise(X, block_size=1000):
    N = X.shape[0]
    X = X.astype(np.float32)

    # Normalisation des vecteurs ligne de X
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    X = X / (norms + 1e-8)  # pour √©viter la division par z√©ro

    # Matrice de sortie
    S = np.empty((N, N), dtype=np.float32)

    for i in range(0, N, block_size):
        Xi = X[i:min(i+block_size, N)]
        for j in range(0, N, block_size):
            Xj = X[j:min(j+block_size, N)]
            S_block = np.dot(Xi, Xj.T)
            S[i:i+Xi.shape[0], j:j+Xj.shape[0]] = S_block

    # transformation de la similarit√© cosine en une similarit√© comprise entre 0 et 1 
    S = 0.5 * (1.0 + S)
    S = np.clip(S, 0.0, 1.0)
    # Diagonale √† 1.0 (au cas o√π il y aurait un flottement num√©rique)
    np.fill_diagonal(S, 1.0)
    return S

# Fonction propos√©e par chat GPT pour optimiser √† la fois alpha et n_cluster dans le cas de clustering spectral non supervis√© : 
def grid_search_alpha_k(A, Scosine, 
                        alphas=np.linspace(0, 1, 11), 
                        k_range=range(2, 11), 
                        metric='silhouette', 
                        n_jobs=-1, verbose=False):
    
    from sklearn.cluster import SpectralClustering
    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
    from joblib import Parallel, delayed
    import numpy as np

    assert metric in ['silhouette', 'calinski', 'davies'], "metric doit √™tre 'silhouette', 'calinski' ou 'davies'"
    results = []
    best_result = None

    def evaluate(alpha, k):
        S = alpha * A + (1 - alpha) * Scosine
        try:
            model = SpectralClustering(n_clusters=k, affinity='precomputed', assign_labels='kmeans')
            y_pred = model.fit_predict(S)

            if metric == 'silhouette':
                score = silhouette_score(1 - S, y_pred, metric='precomputed')
            elif metric == 'calinski':
                score = calinski_harabasz_score(S, y_pred)
            elif metric == 'davies':
                score = -davies_bouldin_score(S, y_pred)  # on inverse car plus petit = meilleur

            if verbose:
                print(f"[Œ±={alpha:.2f}, k={k}] {metric} = {score:.3f}")

            return {'alpha': alpha, 'k': k, 'score': score, 'y_pred': y_pred, 'S': S}
        
        except Exception as e:
            if verbose:
                print(f"[Œ±={alpha:.2f}, k={k}] Erreur : {e}")
            return None

    tasks = [(alpha, k) for alpha in alphas for k in k_range]

    all_results = Parallel(n_jobs=n_jobs)(
        delayed(evaluate)(alpha, k) for alpha, k in tasks
    )
    all_results = [r for r in all_results if r is not None]

    best_result = max(all_results, key=lambda r: r['score'])

    print(f"\n‚úÖ Meilleur : alpha={best_result['alpha']:.2f}, k={best_result['k']}, score={best_result['score']:.3f}")

    return (
        all_results, 
        best_result['alpha'], 
        best_result['k'], 
        best_result['score'], 
        best_result['y_pred'], 
        best_result['S']  # üëà matrice S du meilleur alpha
    )


'''    
# fonction pour optimiser alpha dans le cadre d'un clustering spectral supervis√©
def optimize_alpha_spectral(A, Scosine, y, alphas=np.linspace(0, 1, 11), metric='ARI'):
    assert metric in ['ARI', 'NMI'], "metric doit √™tre 'ARI' ou 'NMI'"
    n_clusters = len(np.unique(y[y != -1]))
    results = []
    best_result = None

    for alpha in alphas:
        S = alpha * A + (1 - alpha) * Scosine
        model = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')
        y_pred = model.fit_predict(S)

        mask = y != -1
        ari = adjusted_rand_score(y[mask], y_pred[mask])
        nmi = normalized_mutual_info_score(y[mask], y_pred[mask])

        print(f"[Œ±={alpha:.2f}] ARI={ari:.3f} | NMI={nmi:.3f}")
        result = {'alpha': alpha, 'ARI': ari, 'NMI': nmi, 'y_pred': y_pred}
        results.append(result)

        if best_result is None or result[metric] > best_result[metric]:
            best_result = result

    print(f"\n‚úÖ Meilleur alpha (selon {metric}) : {best_result['alpha']:.2f} ‚Üí {metric} = {best_result[metric]:.3f}")

    return results, best_result['alpha'], best_result[metric], best_result['y_pred']
'''

# Boucle sur tous les datasets
for dataset_name, g in graphs_modif.items():
    # ================================
    # 1. Extraction des features des n≈ìuds
    # ================================
    # Passage des features en numpy pour export
    x = g.ndata['feature'].cpu().numpy()

    # ================================
    # 2. Extraction des √©tiquettes des n≈ìuds
    # ================================
    if 'label' in g.ndata:
        # Si les labels sont pr√©sents, on les extrait
        y = g.ndata['label'].cpu().numpy()
    else:
        # Sinon, on utilise -1 pour indiquer l'absence d'√©tiquette
        y = np.full(g.num_nodes(), fill_value=-1)                       # EST CE QUE CE TRUC LA EST UNE PRATIQUE OK ???

    # ================================
    # 3. Cr√©ation de la matrice de poids des ar√™tes
    # ================================
    num_nodes = g.num_nodes()

    # Initialisation d'une matrice (num_nodes x num_nodes) remplie de z√©ros
    A = np.zeros((num_nodes, num_nodes))

    # R√©cup√©ration des ar√™tes (liste des paires source ‚Üí destination)
    src, dst = g.edges()
    src = src.cpu().numpy()
    dst = dst.cpu().numpy()

    # Si les ar√™tes ont un attribut 'count' (poids des ar√™tes), on l‚Äôutilise ; sinon poids unitaire
    if 'count' in g.edata:
        count = g.edata['count']         # tensor shape: [N, 1]
        count = count.squeeze()          # shape devient [N]
        count = count.cpu().numpy()      # devient array([1., 2., ...])
    else:
        count = np.ones(len(src))  # poids par d√©faut = 1

    # Remplissage de la matrice de similarit√©s avec les poids
    for s, d, w in zip(src, dst, count):
        A[s, d] = w
        # A[d, s] = w  # si le graphe est non orient√© (sym√©trique)
    print(f"matrice d'adjacence : {A}")

    # ================================
    # 4. Cr√©ation de la matrice Scosine de similarit√© des features des noeuds (pour le clustering spectral ici 
    # - pour le clustering hyperbolique : ce sera fait dans HypHC)
    # ================================

    # j'ai x (numpy) la matrice des features des noeuds

    print("Min global :", x.min())
    print("Max global :", x.max())
    
    norms = np.linalg.norm(x, axis=1)
    print("Norme moyenne :", norms.mean())
    print("Norme max :", norms.max())

    x = x / np.clip(np.linalg.norm(x, axis=1, keepdims=True), 1e-8, None)

    norms = np.linalg.norm(x, axis=1)
    print("Nouvelle norme moyenne :", norms.mean())
    print("Nouvelle norme max :", norms.max())

    # Calcul de la similarit√© cosine entre features des noeuds par blocs (pour ne pas exploser la m√©moire dispo)
    Scosine = compute_cosine_similarity_matrix_blockwise(x, block_size=1000)

    Scosine = np.exp(Scosine * 10)  # accentue les diff√©rences car sinon nos Scosine sont tr√®s "plates" (tout s'y ressemble !)

    # ================================
    # 5. Cr√©ation de la matrice similarities, qui combine poids des ar√™tes et similarit√©s entre features des noeuds,
    # avec un hyperparam√®tre alpha √† optimiser.
    # On le fait ici uniquement pour le clustering spectral (pour le clustering hyperbolique, on va juste exporter A,
    # et on construira similarities dans HypHC directement)
    # ================================
    
    results, alpha_opt, k_opt, score_opt, y_opt, similarities = grid_search_alpha_k(
        A, Scosine,
        alphas=np.linspace(0, 1, 11),   # ou par ex. np.linspace(0.2, 0.8, 7)
        k_range=range(2, 11),           # k = nombre de clusters √† tester
        metric='silhouette',           # ou 'calinski' ou 'davies'
        n_jobs=-1,                      # pour utiliser tous les c≈ìurs CPU
        verbose=True                    # pour afficher l‚Äôavancement
        )

    ###################################

    # visualisation de A et Scosine

    ##################################

    def plot_similarity_matrices(A, Scosine, filename="matrices_similarite.png"):
        plt.figure(figsize=(10, 4))

        plt.subplot(1, 2, 1)
        plt.imshow(A, cmap='viridis')
        plt.title("A")

        plt.subplot(1, 2, 2)
        plt.imshow(Scosine, cmap='viridis')
        plt.title("Scosine")

        plt.tight_layout()
        plt.savefig(filename)
        plt.close()  # ferme proprement la figure pour √©viter les fuites m√©moire

    plot_similarity_matrices(A, Scosine, f"matrices_{dataset_name}.png")


    # ================================
    # 6. Sauvegarde en S3 sur le cloud du datalab INSEE
    # ================================

    BUCKET = "sophieperrinlyon2"
    PREFIX = "albert/"

    fs = s3fs.S3FileSystem()

    for name, arr in [(f"x_{dataset_name}.npy", x), (f"y_{dataset_name}.npy", y), (f"A_{dataset_name}.npy", A)]:
        path = f"{BUCKET}/{PREFIX}{name}"
        with fs.open(path, "wb") as f:
            np.save(f, arr)
            print(f"  ‚úî Uploaded {name}")
    print(dataset_name, g.num_nodes(), g.num_edges())

