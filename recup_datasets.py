
# import argparse
import utils as ut
import os
import s3fs
import numpy as np
import dgl
import networkx as nx
import pickle
import warnings
import torch
from torch import sparse
import numpy as np
warnings.filterwarnings("ignore")
seed_list = list(range(3407, 10000, 10))

import torch
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

#############################################

# Fonctions pour l'Ã©tude des caractÃ©ristiques des graphes de donnÃ©es

#############################################


def analyser_arÃªtes(g, poids_key='count'):
    src, dst = g.edges()
    weights = g.edata[poids_key]

    num_edges = len(src)
    print(f"\nğŸ”¢ Nombre total dâ€™arÃªtes : {num_edges}")

    # Auto-boucles et arÃªtes entre nÅ“uds diffÃ©rents
    mask_self = src == dst
    mask_diff = src != dst

    weights_self = weights[mask_self]
    weights_diff = weights[mask_diff]

    print(f"ğŸ” Auto-boucles : {len(weights_self)} arÃªtes, poids min = {weights_self.min().item() if len(weights_self) > 0 else None}, max = {weights_self.max().item() if len(weights_self) > 0 else None}")
    print(f"ğŸ”— ArÃªtes entre nÅ“uds diffÃ©rents : {len(weights_diff)} arÃªtes, poids min = {weights_diff.min().item() if len(weights_diff) > 0 else None}, max = {weights_diff.max().item() if len(weights_diff) > 0 else None}")

    # Conversion CPU pour comparaison set
    src_np = src.cpu().numpy()
    dst_np = dst.cpu().numpy()
    weights_np = weights.cpu().numpy()

    edges_np = np.stack((src_np, dst_np), axis=1)
    edge_set = set(map(tuple, edges_np))

    sym_mask = np.array([(j, i) in edge_set for i, j in edges_np])
    asym_mask = ~sym_mask

    weights_sym = weights_np[sym_mask]
    weights_asym = weights_np[asym_mask]

    print(f"ğŸ”„ ArÃªtes avec arÃªte inverse : {len(weights_sym)} arÃªtes, poids min = {weights_sym.min() if len(weights_sym) > 0 else None}, max = {weights_sym.max() if len(weights_sym) > 0 else None}")
    print(f"â†ªï¸ ArÃªtes sans arÃªte inverse : {len(weights_asym)} arÃªtes, poids min = {weights_asym.min() if len(weights_asym) > 0 else None}, max = {weights_asym.max() if len(weights_asym) > 0 else None}")


def describe_dgl_graph(g, name, max_examples=5):
    print(f"ğŸ“Š RÃ©sumÃ© du graphe DGL du jeu de donnÃ©es {name}")
    print("-" * 40)
    print(f"Nombre de nÅ“uds : {g.num_nodes()}")
    print(f"Nombre d'arÃªtes : {g.num_edges()}")
    print(g)

    nxg = g.to_networkx()
    print("Le graphe est-il orientÃ© ?", nxg.is_directed())

    # Affichage de quelques infos de base
    print(f"Node feature shape: {g.ndata['feature'].shape}")
    print(f"Label present: {'label' in g.ndata}")
    
    print("\nğŸ”‘ Attributs des nÅ“uds :")
    for key in g.ndata.keys():
        print(f" - {key}: shape = {g.ndata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.ndata[key][:max_examples]}")
    
    print("\nğŸ”‘ Attributs des arÃªtes :")
    for key in g.edata.keys():
        print(f" - {key}: shape = {g.edata[key].shape}")
        if max_examples > 0:
            print(f"   Exemple : {g.edata[key][:max_examples]}")

    print("\nğŸ§ª Masques (sâ€™ils existent) :")
    for mask in ['train_mask', 'val_mask', 'test_mask']:
        if mask in g.ndata:
            print(f" - {mask} â†’ {g.ndata[mask].sum().item()} nÅ“uds")
    
    print("\nğŸ” Quelques arÃªtes :")
    src, dst = g.edges()
    for i in range(min(max_examples, len(src))):
        print(f"   {src[i].item()} â†’ {dst[i].item()}")
    '''
    print(g.etypes)
    for etype in g.canonical_etypes:
        print(f"{etype} features:", g.edges[etype].data.keys())
    print(g.edges[('_N', '_E', '_N')].data['count'])
    for etype in g.canonical_etypes:
        for key in g.edges[etype].data.keys():
            print(f"{etype} - {key}:")
            print(g.edges[etype].data[key])

    src, dst = g.edges(form='uv', etype=('_N', '_E', '_N'))
    counts = g.edges[('_N', '_E', '_N')].data['count']

    for i, (u, v, c) in enumerate(zip(src, dst, counts)):
        if i >= 100:
            break
        print(f"{u.item()} -> {v.item()} : count = {c.item()}")
    '''
    '''
    # 1. RÃ©cupÃ©rer les arÃªtes et la feature 'count'
    src, dst = g.edges(form='uv', etype=('_N', '_E', '_N'))
    counts = g.edges[('_N', '_E', '_N')].data['count']          # (num_edges, 1) ou (num_edges,)

# 2. Construire un masque boolÃ©en : True si count > 1
    mask = (counts.squeeze() > 1)   # .squeeze() enlÃ¨ve la dimension inutile si besoin

# 3. Appliquer le masque pour ne garder que les arÃªtes voulues
    src_keep = src[mask]
    dst_keep = dst[mask]
    count_keep = counts[mask]

# 4. Afficher les arÃªtes filtrÃ©es
    for u, v, c in zip(src_keep, dst_keep, count_keep):
        print(f"{u.item()} -> {v.item()} : count = {c.item()}")
    
    for ntype in g.ntypes:
        print(f"Attributs des nÅ“uds de type {ntype} :")
        print(list(g.nodes[ntype].data.keys()))

    '''

    print(f"Le graphe est-il homogÃ¨ne ? {g.is_homogeneous}")
    print(f"Le graphe est-il unibipartite ? {g.is_unibipartite}")
    print(f"RÃ©sultats de has nodes : {g.has_nodes}")

# Autre mÃ©thode pour matrice d'adjacence
    # Construire la matrice sparse pondÃ©rÃ©e
    # Si src et dst sont des numpy arrays, on les convertit en torch tensors
    src_tensor = torch.tensor(src) if not torch.is_tensor(src) else src
    dst_tensor = torch.tensor(dst) if not torch.is_tensor(dst) else dst
    if 'count' in g.edata:
        count = g.edata['count']         # tensor shape: [N, 1]
        count = count.squeeze()          # shape devient [N]
    
    num_nodes = g.num_nodes()

    adj = torch.sparse_coo_tensor(
        indices=torch.stack([src_tensor, dst_tensor]),
        values=count,
        size=(num_nodes, num_nodes)
    )

# adj contient les poids des arÃªtes
    print(f"matrice d'adjacence autre mÃ©thode de calcul : {adj.to_dense()}")  # Affiche la matrice dense
    # Si adj est sparse
    dense_adj = adj.to_dense()

    # Convertir en numpy array
    np_adj = dense_adj.cpu().numpy()
    if np.allclose(np_adj.T,np_adj):
        print("matrice symÃ©trique")
    else:
        print("matrice pas symÃ©trique")

###
    src, dst = g.edges()
    num_edges = len(src)

# ArÃªtes entre nÅ“uds diffÃ©rents
    mask_diff_nodes = src != dst
    num_diff_edges = mask_diff_nodes.sum().item()

    print(f"ğŸ”¢ Nombre total d'arÃªtes : {num_edges}")
    print(f"ğŸ”— ArÃªtes entre nÅ“uds diffÃ©rents : {num_diff_edges}")
    print(f"ğŸ” Auto-boucles (i â†’ i) : {num_edges - num_diff_edges}")

    weights = g.edata['count']
    min_w = weights.min().item()
    max_w = weights.max().item()

    print(f"ğŸ“ Plage des poids dâ€™arÃªtes : min = {min_w}, max = {max_w}")

    # CrÃ©er un ensemble des paires (i, j) et (j, i)
    edge_set = set(zip(src.tolist(), dst.tolist()))
    reverse_set = set((j, i) for (i, j) in edge_set if i != j)

# ArÃªtes qui n'ont pas leur inverse
    asym_edges = reverse_set - edge_set
    print(len(asym_edges))
    if len(asym_edges) == 0:
        print("âœ… Le graphe est symÃ©trique : pour chaque arÃªte i â†’ j, il existe j â†’ i.")
    else:
        print(f"âš ï¸ Le graphe est orientÃ© : {len(asym_edges)} arÃªtes nâ€™ont pas leur inverse.")
    analyser_arÃªtes(g)

#############################################

# Fonctions pour la rÃ©duction de dimension des features des noeuds des graphes (lorsque utile)
# et pour transformer un graphe rÃ©ellement orientÃ© en graphe non orientÃ© (par la mÃ©thode de repondÃ©ration des arcs en arÃªtes)

#############################################


def analyze_feature_redundancy(graph, variance_thresh=1e-6, corr_thresh=0.95, pca_variance=0.95):
    # 1. Extraire les features
    X = graph.ndata['feature'].numpy()

    # 2. Calculer la variance
    variances = X.var(axis=0)
    var_idx = np.where(variances >= variance_thresh)[0]       # indices Ã  garder
    low_var_idx = np.where(variances < variance_thresh)[0]    # indices supprimÃ©s pour traÃ§abilitÃ©
    print(f"{len(low_var_idx)} features ont une variance < {variance_thresh} : {low_var_idx.tolist()} â€” elles sont supprimÃ©es avant la PCA")

    # 3. Filtrer les colonnes Ã  faible variance
    X_clean = X[:, var_idx]

    # 4. Mettre Ã  jour les features du graphe
    graph.ndata['feature'] = torch.tensor(X_clean, dtype=torch.float32)

    # 5. Features trÃ¨s corrÃ©lÃ©es (calculÃ© sur X d'origine, pas X_clean)
    corr_matrix = np.corrcoef(X, rowvar=False)
    np.fill_diagonal(corr_matrix, 0)
    
    # Matrice de corrÃ©lation absolue
    abs_corr = np.abs(corr_matrix)

    # Indices de la partie triangulaire supÃ©rieure (hors diagonale)
    triu_indices = np.triu_indices_from(abs_corr, k=1)

    # Paires (i, j) avec leur valeur de corrÃ©lation
    pair_scores = [(i, j, abs_corr[i, j]) for i, j in zip(*triu_indices)]  # ğŸ”„ remplacÃ© redundant_pairs

    # Trier par corrÃ©lation dÃ©croissante
    pair_scores.sort(key=lambda x: x[2], reverse=True)  # ğŸ”„ nouveau : trie toutes les paires par corrÃ©lation

    # Affichage
    print("Top 10 des paires de features les plus corrÃ©lÃ©es :")  # ğŸ”„ message plus clair
    for i, j, score in pair_scores[:10]:                         # ğŸ”„ on affiche les paires rÃ©ellement les plus corrÃ©lÃ©es
        print(f"  Feature {i} â†” Feature {j} (corr = {corr_matrix[i, j]:.2f})")

    # 6. PCA sur les features nettoyÃ©es
    pca = PCA(n_components=pca_variance)
    pca.fit(X_clean)
    print(f"PCA a rÃ©duit de {X_clean.shape[1]} Ã  {pca.n_components_} dimensions (variance expliquÃ©e : {pca_variance})")

    # 7. Affichage des poids de la premiÃ¨re composante
    comp_weights = np.abs(pca.components_[0])
    plt.bar(np.arange(len(comp_weights)), comp_weights)
    plt.title("Poids absolus des features dans la 1re composante principale")
    plt.xlabel("Feature index")
    plt.ylabel("Poids")
    plt.show()

    # 8. Remplacer les features du graphe par celles transformÃ©es par la PCA
    X_pca = pca.transform(X_clean)
    graph.ndata['feature'] = torch.tensor(X_pca, dtype=torch.float32)


    # 9. Retourner les rÃ©sultats
    return {
        'features_supprimÃ©es_par_variance': low_var_idx.tolist(),  
        'top_corr_pairs': pair_scores[:10],                   
        'pca_model': pca,
        'graph_pca': graph 
    }


def make_weighted_undirected_with_node_features(g):
    # 1. Extraire les arÃªtes orientÃ©es
    src, dst = g.edges()

    # 2. Compter les relations (non orientÃ©es)
    edge_counts = {}
    for u, v in zip(src.tolist(), dst.tolist()):
        key = tuple(sorted([u, v]))
        edge_counts[key] = edge_counts.get(key, 0) + 1

    # 3. PrÃ©parer les arÃªtes et les poids
    new_src = []
    new_dst = []
    new_weights = []

    for (u, v), w in edge_counts.items():
        # Attribution du poids basÃ© sur le type de relation
        if u == v:
            weight = 1.0  # boucle
        elif w == 1:
            weight = 0.5  # unidirectionnel
        else:
            weight = 1.0  # bidirectionnel

        # ArÃªte u â†’ v
        new_src.append(u)
        new_dst.append(v)
        new_weights.append(weight)

        # ArÃªte v â†’ u (sauf si boucle)
        if u != v:
            new_src.append(v)
            new_dst.append(u)
            new_weights.append(weight)

    # 4. CrÃ©er le graphe non orientÃ©
    g_undir = dgl.graph((new_src, new_dst), num_nodes=g.num_nodes())

    # 5. Copier les features des nÅ“uds
    for key in g.ndata:
        g_undir.ndata[key] = g.ndata[key].clone()

    # 6. Ajouter les poids aux arÃªtes
    g_undir.edata['count'] = torch.tensor(new_weights, dtype=torch.float)

    return g_undir


#############################################

# #### Etude des caractÃ©ristiques des datasets et crÃ©ation d'un dictionnaire pour pouvoir les manipuler sÃ©parÃ©ment ensuite :

#############################################

datasets = ['reddit', 'weibo']

graphs = {}  # Dictionnaire pour stocker les graphes

# Boucle sur tous les datasets
for dataset_name in datasets:
    # Chargement du dataset avec GADBench
    data = ut.Dataset(name=dataset_name, prefix='GADBench/datasets/')
    g = data.graph  # RÃ©cupÃ©ration du graphe DGL

    graphs[dataset_name] = g  # Stockage du graphe avec son nom

    describe_dgl_graph(g, dataset_name, 2)
    
graphs_modif = {}

#############################################

# Travail de transformation - adaptation du graphe de donnÃ©es reddit

# Ce graphe est symÃ©trique, et chaque arÃªte a une arÃªte inverse. Dans DGL, tous les graphes sont de type orientÃ© (il est impossible qu'ils ne l'y soient pas).
# C'est du Ã  la spÃ©cificitÃ© de DGL : faire des graphes pour y faire tourner des GNN
# Notre graphe, Ã©tant symÃ©trique, est donc dÃ©jÃ  sous la bonne forme en pratique (il n'est orientÃ© que parce que DGL lui attribue ce type)

# On a donc uniquement Ã  se prÃ©occuper des features des noeuds, en Ã©liminant celles qui varient trÃ¨s faiblement entre les diffÃ©rents noeuds,
# puis en effectuant une ACP pour transformer nos features parfois trÃ¨s corrÃ©lÃ©es entre elles, en features orthogonales les unes aux autres, et moins nombreuses

#############################################

resultats_reddit = analyze_feature_redundancy(graphs['reddit'])

graphs_modif['reddit'] = resultats_reddit['graph_pca']

describe_dgl_graph(graphs_modif['reddit'], 'reddit_modif')
#############################################

# Travail de transformation - adaptation du graphe de donnÃ©es weibo

# weibo est rÃ©ellement orientÃ©, car de nombreux arcs n'ont pas d'arc retour : on crÃ©e ces arcs retour
# et on procÃ¨de par repondÃ©ration : 
# 0 pour l'absence totale d'arc entre deux noeuds
# 1 pour un arc A --> B et B --> A
# 0,5 pour un arc uniquement A --> B (sans prÃ©sence d'arc retour dans le graphe d'origine)

#############################################

graphs_modif['weibo'] = make_weighted_undirected_with_node_features(graphs['weibo'])

describe_dgl_graph(graphs_modif['weibo'], 'weibo_modif')

resultats_weibo = analyze_feature_redundancy(graphs['weibo'], variance_thresh=1e-2, corr_thresh=0.95, pca_variance=0.99)

graphs_modif['weibo'] = resultats_weibo['graph_pca']

#############################################
for name, g in graphs_modif.items():
    print(name, g.num_nodes(), g.num_edges())

#############################################

# Export des noeuds+features (x), des labels (y) et de la matrice de similaritÃ© (issue de A remaniÃ©e) des graphes de donnÃ©es

# WARNING : SERA A ADAPTER AUX NOMS DES GRAPHES REMANIES

#############################################

'''
datasets = ['reddit', 'weibo']


# Boucle sur tous les datasets
for dataset_name in datasets:
    # Chargement du dataset avec GADBench
    data = ut.Dataset(name=dataset_name, prefix='GADBench/datasets/')
    g = data.graph  # RÃ©cupÃ©ration du graphe DGL

    describe_dgl_graph(g, dataset_name, 2)

    # ================================
    # 1. Extraction des features des nÅ“uds
    # ================================
    # Passage des features en numpy pour export
    x = g.ndata['feature'].cpu().numpy()

    # ================================
    # 2. Extraction des Ã©tiquettes des nÅ“uds
    # ================================
    if 'label' in g.ndata:
        # Si les labels sont prÃ©sents, on les extrait
        y = g.ndata['label'].cpu().numpy()
    else:
        # Sinon, on utilise -1 pour indiquer l'absence d'Ã©tiquette
        y = np.full(g.num_nodes(), fill_value=-1)

    # ================================
    # 3. CrÃ©ation de la matrice de poids des arÃªtes
    # ================================
    num_nodes = g.num_nodes()

    # Initialisation d'une matrice (num_nodes x num_nodes) remplie de zÃ©ros
    A = np.zeros((num_nodes, num_nodes))

    # RÃ©cupÃ©ration des arÃªtes (liste des paires source â†’ destination)
    src, dst = g.edges()
    src = src.cpu().numpy()
    dst = dst.cpu().numpy()

    # Si les arÃªtes ont un attribut 'count' (poids des arÃªtes), on lâ€™utilise ; sinon poids unitaire
    if 'count' in g.edata:
        count = g.edata['count']         # tensor shape: [N, 1]
        count = count.squeeze()          # shape devient [N]
        count = count.cpu().numpy()      # devient array([1., 2., ...])
    else:
        count = np.ones(len(src))  # poids par dÃ©faut = 1

    # Remplissage de la matrice de similaritÃ©s avec les poids
    for s, d, w in zip(src, dst, count):
        A[s, d] = w
        # A[d, s] = w  # si le graphe est non orientÃ© (symÃ©trique)
    print(f"matrice d'adjacence : {A}")

    BUCKET = "sophieperrinlyon2"
    PREFIX = "albert/"

    fs = s3fs.S3FileSystem()

    for name, arr in [(f"x_{dataset_name}.npy", x), (f"y_{dataset_name}.npy", y), (f"A_{dataset_name}.npy", A)]:
        path = f"{BUCKET}/{PREFIX}{name}"
        with fs.open(path, "wb") as f:
            np.save(f, arr)
            print(f"  âœ” Uploaded {name}")

'''


'''
    # 1. Reconstruire le masque users/items
# Ici j'utilise le fait que seuls les users ont un label >= 0.
    labels = g.ndata['label']               # shape [10984]
    is_user = labels >= 0                   # True = user, False = item
    is_item = ~is_user

# 2. Comptage des nÅ“uds de chaque type
    num_users = is_user.sum().item()
    num_items = is_item.sum().item()
    print(f"Users dÃ©tectÃ©s : {num_users}")
    print(f"Items dÃ©tectÃ©s : {num_items}")

# 3. RÃ©cupÃ©rer toutes les arÃªtes
    src, dst = g.edges()

# 4. Compter les 4 cas de figure
    mask_ui = is_user[src] & is_item[dst]    # user â†’ item
    mask_iu = is_item[src] & is_user[dst]    # item â†’ user
    mask_uu = is_user[src] & is_user[dst]    # user â†’ user
    mask_ii = is_item[src] & is_item[dst]    # item â†’ item

    counts = {
        "userâ†’item" : mask_ui.sum().item(),
        "itemâ†’user" : mask_iu.sum().item(),
        "userâ†’user" : mask_uu.sum().item(),
        "itemâ†’item" : mask_ii.sum().item(),
        "total edges": src.shape[0]
    }

    for k, v in counts.items():
        print(f"{k:10s} : {v}")

# 5. Pourcentage userâ†’item
    pct_ui = counts["userâ†’item"] / counts["total edges"] * 100
    print(f"\n% userâ†’item  : {pct_ui:.2f}%")
'''
'''
import os
import s3fs
import numpy as np

#BUCKET_OUT = "sophieperrinlyon2"


BUCKET = "sophieperrinlyon2"
PREFIX = "albert/"

fs = s3fs.S3FileSystem()

x_reddit = np.load("x_reddit.npy")
y_reddit = np.load("y_reddit.npy")
A_reddit = np.load("A_reddit.npy")


for name, arr in [("x_reddit.npy", x_reddit), ("y_reddit.npy", y_reddit), ("A_reddit.npy", A_reddit)]:
    path = f"{BUCKET}/{PREFIX}{name}"
    with fs.open(path, "wb") as f:
        np.save(f, arr)
    print(f"  âœ” Uploaded {name}")
'''
'''
    # ================================
    # 4. Sauvegarde dans un fichier .npz compressÃ©
    # ================================
    # Nettoyage du nom de fichier
    graph_name = dataset_name.replace("/", "_")

    # Enregistrement dans un fichier compressÃ© contenant x, y, similarities
    np.savez_compressed(
        os.path.join(output_dir, f"{graph_name}.npz"),
        x=x,
        y=y,
        A=A
    )

'''

'''  
    # sauvegarder les features ou graphes
    X = g.ndata['feature'].cpu().numpy()
    np.save(f"{dataset_name}_features.npy", X)
    # convertir en networkx
    nx_graph = dgl.to_networkx(g)
    with open(f"{dataset_name}.gpickle", "wb") as f:
        pickle.dump(nx_graph, f)

'''



'''  
    # sauvegarder les features ou graphes
    X = g.ndata['feature'].cpu().numpy()
    np.save(f"{dataset_name}_features.npy", X)
    # convertir en networkx
    nx_graph = dgl.to_networkx(g)
    with open(f"{dataset_name}.gpickle", "wb") as f:
        pickle.dump(nx_graph, f)

'''


